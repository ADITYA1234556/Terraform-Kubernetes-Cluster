name: "CICD IAC"
on:
  workflow_dispatch:
  push:
    branches:
      - master
      - stage
  pull_request:
    branches:
      - main

env:
  AWS_ACCESS_KEY_ID: ${{secrets.AWS_ACCESS_KEY_ID}}
  AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_SECRET_ACCESS_KEY}}
  BUCKET_TF_STATE: ${{secrets.BUCKET_TF_STATE}}
  AWS_REGION: eu-west-2
  PEM_KEY: ${{secrets.PEM_KEY}}

jobs:
    terraform:
      name: "Apply code changes"
      runs-on: ubuntu-latest #this is a runner on ubuntu that provides a linux based shell

      steps:
        - name: Checkout source code
          uses: actions/checkout@v4

        - name: Install Terraform on the ubuntu runner to run our terraform scripts
          uses: hashicorp/setup-terraform@v2

        - name: Terraform init
          id: init #id can be any name it will used to build dependencies
          run: terraform init -backend-config="bucket=$BUCKET_TF_STATE"

        - name: Terraform refresh
          id: refresh
          run: terraform refresh

        - name: Terraform format
          id: fmt
          run: terraform fmt

        - name: Terraform validate
          id: validate
          run: terraform validate

        - name: Terraform plan
          id: plan
          run: terraform plan -no-color -input=false -out planfile
          continue-on-error: true

        - name: Terraform plan status #here we specify when to exit the code
          if: steps.plan.outcome == 'failure'
          run: exit 1

        - name: Install jq
          run: |
            sudo apt-get install -y jq
            jq --version

        - name: Terraform apply the code
          id: apply #apply only if there is a push event in main branch
          #if: github.ref == 'refs/heads/master' && github.event_name == 'push'
          run: |
            terraform apply -auto-approve -input=false -parallelism=1 planfile 
            #we are using the planfile we got from the output in step with id plan
            # Fetch the public_ips output as JSON
            output_json=$(terraform output -json public_ips)
          
            # Debug output JSON for troubleshooting
            echo "Terraform Output (public_ips): $output_json"
          
            # Use jq to parse the public IPs from the JSON
            # Wrap the jq command inside quotes to ensure correct parsing
            public_ips=$(echo "$output_json" | jq -r '.[]')
          
            # Check if the public_ips is empty or not parsed properly
            if [[ -z "$public_ips" ]]; then
              echo "Error: No public IPs found. Exiting."
              exit 1
            fi
          
            # Extract the first and second IPs from the list
            FIRST_IP=$(echo "$public_ips" | head -n1)
            SECOND_IP=$(echo "$public_ips" | tail -n1)
          
            # Output the results to GitHub environment variables
            echo "FIRST_IP=$FIRST_IP" >> $GITHUB_ENV
            echo "SECOND_IP=$SECOND_IP" >> $GITHUB_ENV
          
            echo "First IP: $FIRST_IP, Second IP: $SECOND_IP"

        - name: Configure AWS credentials #setting up aws credentials so that we can get the kubeconfig file in the next step
          uses: aws-actions/configure-aws-credentials@v1
          with:
            aws-access-key-id: ${{secrets.AWS_ACCESS_KEY_ID}}
            aws-secret-access-key: ${{secrets.AWS_SECRET_ACCESS_KEY}}
            aws-region: ${{ env.AWS_REGION}}

         # SSH into the first EC2 instance and initialize the Kubernetes cluster
        - name: Initialize Kubernetes on Master Node
          id: init_master
          env:
            PEM_KEY: ${{ secrets.PEM_KEY }} # Ensure PEM key is stored in GitHub Secrets
            FIRST_IP: ${{ env.FIRST_IP }}
          run: |
            echo "${PEM_KEY}" > private.pem
            chmod 600 private.pem
            ssh -o StrictHostKeyChecking=no -i private.pem ubuntu@$FIRST_IP \
              "sudo kubeadm init --pod-network-cidr=10.244.0.0/16" > init_output.txt
            JOIN_COMMAND=$(grep -o 'kubeadm join.*' init_output.txt)
            echo "JOIN_COMMAND=$JOIN_COMMAND" >> $GITHUB_ENV

        # Step 5: SSH into the second EC2 instance and join the cluster
        - name: Join Worker Node to Cluster
          env:
            PEM_KEY: ${{ secrets.PEM_KEY }}
            SECOND_IP: ${{ env.SECOND_IP }}
            JOIN_COMMAND: ${{ env.JOIN_COMMAND }}
          run: |
            echo "${PEM_KEY}" > private.pem
            chmod 600 private.pem
            ssh -o StrictHostKeyChecking=no -i private.pem ubuntu@$SECOND_IP \
              "sudo $JOIN_COMMAND"


